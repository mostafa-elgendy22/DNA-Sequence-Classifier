{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cae6893a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix, roc_auc_score, roc_curve, auc, precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "837bdc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\"../Saved Data/Model\")\n",
    "X_test = np.load('../Saved Data/Test Set/X_test.npy')\n",
    "y_test = np.load('../Saved Data/Test Set/y_test.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05dc33b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - 0s 7ms/step\n"
     ]
    }
   ],
   "source": [
    "probabilities_vector = model.predict(X_test)\n",
    "y_pred = np.argmax(probabilities_vector, axis=1)\n",
    "np.savetxt('../Saved Data/Predicted Labels/python_model.txt', y_pred, fmt='%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4aab9c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def roc_auc(y_true, y_pred, y_score=None, average='micro'):\n",
    "    if y_true.shape != y_pred.shape:\n",
    "        print(\"Error! y_true %s is not the same shape as y_pred %s\" % (\n",
    "              y_true.shape,\n",
    "              y_pred.shape)\n",
    "        )\n",
    "        return\n",
    "\n",
    "    lb = LabelBinarizer()\n",
    "\n",
    "    if len(y_true.shape) == 1:\n",
    "        lb.fit(y_true)\n",
    "\n",
    "    #Value counts of predictions\n",
    "    labels, cnt = np.unique(\n",
    "        y_pred,\n",
    "        return_counts=True)\n",
    "    n_classes = len(labels)\n",
    "    pred_cnt = pd.Series(cnt, index=labels)\n",
    "\n",
    "    # metrics_summary = precision_recall_fscore_support(\n",
    "    #         y_true=y_true,\n",
    "    #         y_pred=y_pred,\n",
    "    #         labels=labels)\n",
    "\n",
    "    # avg = list(precision_recall_fscore_support(\n",
    "    #         y_true=y_true, \n",
    "    #         y_pred=y_pred,\n",
    "    #         average='weighted'))\n",
    "\n",
    "    # metrics_sum_index = ['precision', 'recall', 'f1-score', 'support']\n",
    "    # class_report_df = pd.DataFrame(\n",
    "    #     list(metrics_summary),\n",
    "    #     index=metrics_sum_index,\n",
    "    #     columns=labels)\n",
    "\n",
    "    # support = class_report_df.loc['support']\n",
    "    # total = support.sum() \n",
    "    # class_report_df['avg / total'] = avg[:-1] + [total]\n",
    "\n",
    "    # class_report_df = class_report_df.T\n",
    "    # class_report_df['pred'] = pred_cnt\n",
    "    # class_report_df['pred'].iloc[-1] = total\n",
    "\n",
    "    if not (y_score is None):\n",
    "        fpr = dict()\n",
    "        tpr = dict()\n",
    "        roc_auc = dict()\n",
    "        for label_it, label in enumerate(labels):\n",
    "            fpr[label], tpr[label], _ = roc_curve(\n",
    "                (y_true == label).astype(int), \n",
    "                y_score[:, label_it])\n",
    "\n",
    "            roc_auc[label] = auc(fpr[label], tpr[label])\n",
    "\n",
    "        if average == 'micro':\n",
    "            if n_classes <= 2:\n",
    "                fpr[\"avg / total\"], tpr[\"avg / total\"], _ = roc_curve(\n",
    "                    lb.transform(y_true).ravel(), \n",
    "                    y_score[:, 1].ravel())\n",
    "            else:\n",
    "                fpr[\"avg / total\"], tpr[\"avg / total\"], _ = roc_curve(\n",
    "                        lb.transform(y_true).ravel(), \n",
    "                        y_score.ravel())\n",
    "\n",
    "            roc_auc[\"avg / total\"] = auc(\n",
    "                fpr[\"avg / total\"], \n",
    "                tpr[\"avg / total\"])\n",
    "\n",
    "        elif average == 'macro':\n",
    "            # First aggregate all false positive rates\n",
    "            all_fpr = np.unique(np.concatenate([\n",
    "                fpr[i] for i in labels]\n",
    "            ))\n",
    "\n",
    "            # Then interpolate all ROC curves at this points\n",
    "            mean_tpr = np.zeros_like(all_fpr)\n",
    "            for i in labels:\n",
    "                mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "            # Finally average it and compute AUC\n",
    "            mean_tpr /= n_classes\n",
    "\n",
    "            fpr[\"macro\"] = all_fpr\n",
    "            tpr[\"macro\"] = mean_tpr\n",
    "\n",
    "            roc_auc[\"avg / total\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "\n",
    "    return roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1724d87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score = accuracy_score(y_test, y_pred)\n",
    "precision_score = precision_score(y_test, y_pred, average='macro')\n",
    "recall_score = recall_score(y_test, y_pred, average='macro')\n",
    "f1_score = f1_score(y_test, y_pred, average='macro')\n",
    "confusion_matrix = confusion_matrix(y_test, y_pred)\n",
    "classification_report = classification_report(y_test, y_pred)\n",
    "AUC_score = roc_auc(y_test, y_pred, probabilities_vector, average='macro')['avg / total']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca4e2f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUC_scores = roc_auc_score(y_test, probabilities_vector, average=None, multi_class='ovr')\n",
    "# print(AUC_scores)\n",
    "\n",
    "# AUC_scores = roc_auc(y_test, y_pred, probabilities_vector, average='macro')\n",
    "# AUC_scores = list(AUC_scores.values())\n",
    "# AUC_scores.pop()\n",
    "# AUC_scores = [round(x, 8) for x in AUC_scores]\n",
    "# print(AUC_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de199a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists('../Saved Data/Temp Data/python_model.json'):\n",
    "    os.remove('../Saved Data/Temp Data/python_model.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0f73e03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Accuracy = 79.11%\n",
      "Precision = 84.51%\n",
      "Recall = 73.9%\n",
      "F1 Score = 77.08%\n",
      "AUC Score = 0.97\n"
     ]
    }
   ],
   "source": [
    "data = {\"accuracy\": accuracy_score, \n",
    "        \"precision\": precision_score, \n",
    "        \"recall\": recall_score, \n",
    "        \"f1\": f1_score, \n",
    "        \"auc\": AUC_score\n",
    "        }\n",
    "json_data = json.dumps(data, indent = 4)\n",
    "json_file = open('../Saved Data/Temp Data/python_model.json', 'w')\n",
    "json_file.write(json_data)\n",
    "print(f\"Classification Accuracy = {round(accuracy_score * 100, 2)}%\\nPrecision = {round(precision_score * 100, 2)}%\\nRecall = {round(recall_score * 100, 2)}%\\nF1 Score = {round(f1_score * 100, 2)}%\\nAUC Score = {round(AUC_score, 2)}\")\n",
    "json_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d21e5a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.80      0.81        93\n",
      "           1       0.68      0.88      0.77       107\n",
      "           2       0.94      0.73      0.82        89\n",
      "           3       0.92      0.68      0.79       139\n",
      "           4       0.92      0.53      0.67       123\n",
      "           5       0.90      0.58      0.70        45\n",
      "           6       0.73      0.98      0.83       280\n",
      "\n",
      "    accuracy                           0.79       876\n",
      "   macro avg       0.85      0.74      0.77       876\n",
      "weighted avg       0.82      0.79      0.79       876\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c75a394e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 74   4   0   0   0   0  15]\n",
      " [  1  94   0   1   0   0  11]\n",
      " [  0   4  65   1   1   0  18]\n",
      " [  2  12   1  95   4   0  25]\n",
      " [  5  19   3   3  65   3  25]\n",
      " [  5   5   0   0   0  26   9]\n",
      " [  2   0   0   3   1   0 274]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "47b0946bdb42e5a8db21df9138f17b5cb1a6507cd769a113b48a94aa211b3a2e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
